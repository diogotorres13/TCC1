# On Evaluating the Efficiency of Source Code Generated by LLMs  

Wang, H., et al. *“On Evaluating the Efficiency of Source Code Generated by LLMs,”* in **IEEE Transactions on Software Engineering**, 2024. doi: [10.1109/TSE.2024.10599574](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10599574)  

## 1. Fichamento de Conteúdo  

Este artigo investiga a eficiência do código-fonte gerado por modelos de linguagem de grande escala (LLMs), considerando não apenas a correção funcional, mas também métricas como tempo de execução, consumo de memória e manutenibilidade. O problema abordado é que, embora os LLMs sejam capazes de gerar soluções sintaticamente corretas, pouco se sabe sobre a qualidade não funcional dessas implementações. A metodologia empregada consistiu na geração automática de soluções para um conjunto de problemas de...

## 2. Fichamento Bibliográfico  

- A avaliação de código gerado por LLMs deve incluir métricas de desempenho e eficiência, não apenas de correção (p. 2).  
- Foram usados benchmarks de programação clássicos para comparar implementações humanas com as sugeridas por LLMs (p. 3).  
- Resultados indicam que, em vários casos, o código gerado é funcionalmente correto, mas menos eficiente em termos de uso de recursos (p. 5).  
- O estudo mostra que há risco de “overhead” significativo quando soluções são usadas sem revisão (p. 6).  
- Recomenda-se o uso de revisões humanas e ferramentas de otimização em conjunto com LLMs (p. 7).  

## 3. Fichamento de Citações  

- *“While large language models can generate syntactically correct code, their efficiency has not been systematically studied.”* (p. 1)  
- *“We evaluate efficiency along multiple dimensions, including runtime performance and memory consumption.”* (p. 3)  
- *“In many cases, LLM-generated solutions are correct but less efficient than human-written implementations.”* (p. 5)  
- *“Developers should treat LLMs as assistants rather than replacements, especially when efficiency is critical.”* (p. 6)  
- *“Our findings highlight the importance of incorporating efficiency-oriented evaluation in future LLM benchmarks.”* (p. 7)  
